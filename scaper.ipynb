{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import ChromeOptions\n",
    "import pandas as pd\n",
    "import time\n",
    "import threading\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.options =  ChromeOptions()\n",
    "        self.prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "        self.options.add_experimental_option(\"prefs\",self.prefs)\n",
    "        self.options.add_argument(\"start-maximised\")\n",
    "        self.options.add_argument(\"--no-sandbox\")\n",
    "        self.driver = webdriver.Chrome(options=self.options,executable_path='C:/Users/Udit/Downloads/chromedriver')\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.data = {'id': [], 'title': [], 'description': [], 'category': []}\n",
    "        self.links = []\n",
    "        self.temp = []\n",
    "        \n",
    "    def get_links_youtube(self,category):\n",
    "        \n",
    "        search_text = category\n",
    "        if len(category.split(\" \")) > 1:\n",
    "            search_text = category.split(\" \")[0]\n",
    "            for text in category.split(\" \")[1:]:\n",
    "                search_text+= \"+\"+text\n",
    "        print(search_text)\n",
    "        self.driver.get('https://www.youtube.com/results?search_query='+search_text)\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//div[@id='contents'and @class='style-scope ytd-item-section-renderer']\")))\n",
    "        except TimeoutException:\n",
    "            self.driver.get('https://www.youtube.com/results?search_query='+category)\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//div[@id='contents'and @class='style-scope ytd-item-section-renderer']\")))\n",
    "            \n",
    "            \n",
    "        last_height = len(self.driver.find_element_by_xpath(\"//div[@class='style-scope ytd-item-section-renderer' and @id='contents']\").find_elements_by_tag_name(\"ytd-video-renderer\"))\n",
    "        #print(last_height)\n",
    "        while True:\n",
    "            self.driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(3)\n",
    "            self.driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            new_height = len(self.driver.find_element_by_xpath(\"//div[@class='style-scope ytd-item-section-renderer' and @id='contents']\").find_elements_by_tag_name(\"ytd-video-renderer\"))\n",
    "            if new_height == last_height:\n",
    "                print(new_height,last_height,\"youtube stopped scrollign!\")\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "        contents = self.driver.find_element_by_xpath(\"//div[@id='contents'and @class='style-scope ytd-item-section-renderer']\").find_elements_by_xpath(\"ytd-video-renderer\")    \n",
    "\n",
    "        for content in contents:    \n",
    "            url = content.find_element_by_id(\"video-title\").get_attribute(\"href\")\n",
    "            #print(url)\n",
    "            data_link = (url,category,'youtube')\n",
    "            self.links.append(data_link)\n",
    "            \n",
    "            \n",
    "    def get_links_dailymotion(self,category):\n",
    "        self.driver.get('https://www.dailymotion.com/search/' + category + '/videos')\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='Grid Grid__grid___U2CEO']\")))\n",
    "        except TimeoutException:\n",
    "            self.driver.get('https://www.dailymotion.com/search/' + category + '/videos')\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='Grid Grid__grid___U2CEO']\")))\n",
    "            \n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            self.driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(3)\n",
    "            self.driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(3)\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                print(new_height,last_height,\"dailymotion stopped scrolling!\")\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "            contents = self.driver.find_element_by_xpath(\"//div[@class='Grid Grid__grid___U2CEO']\")\n",
    "            values = contents.find_element_by_tag_name(\"div\")\n",
    "            section = values.find_elements_by_xpath(\"//section[@class='Video__wrap___2atEf Video__video___2Qq1K Video__smallHorizontal___2qSHD']\")\n",
    "            for video_data in section:\n",
    "                url = video_data.find_element_by_tag_name(\"a\").get_attribute(\"href\")\n",
    "                if url not in self.temp:\n",
    "                    self.temp.append(url)\n",
    "                    link_data = (url,category,'dailymotion')\n",
    "                    self.links.append(link_data)\n",
    "            \n",
    "    \n",
    "    def get_data(self):\n",
    "        for data_link in self.links:\n",
    "            link, category, source = data_link\n",
    "            #print(data_link)\n",
    "            if source == 'youtube':\n",
    "                self.driver.get(link)\n",
    "                time.sleep(2)\n",
    "                try:\n",
    "                    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"h1.title yt-formatted-string\")))\n",
    "                except :\n",
    "                    self.driver.get(self.links[self.links.index(link) + 1])\n",
    "                    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"h1.title yt-formatted-string\")))\n",
    "                time.sleep(3)\n",
    "                self.data['id'].append(link.split(\"=\")[-1])\n",
    "                self.data['category'].append(category)\n",
    "                self.data['title'].append(self.driver.find_element_by_xpath(\"//h1[@class='title style-scope ytd-video-primary-info-renderer']\").text)\n",
    "                self.data['description'].append(self.driver.find_element_by_xpath(\"//div[@class='style-scope ytd-video-secondary-info-renderer' and @id='description']\").text)\n",
    "                \n",
    "            if source == 'dailymotion':\n",
    "                self.driver.get(link)\n",
    "                time.sleep(2)\n",
    "                try:\n",
    "                    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.Root__page___1mPV6\")))\n",
    "                except:\n",
    "                    self.driver.get(self.links[self.links.index(link) + 1])\n",
    "                    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.Root__page___1mPV6\")))\n",
    "                time.sleep(5)\n",
    "                content = self.driver.find_element_by_css_selector(\"div.Root__page___1mPV6\")\n",
    "                wait_1 = WebDriverWait(content,5)\n",
    "                wait_1.until(EC.presence_of_element_located((By.CLASS_NAME,\"VideoInfoDescription__descriptionText___RB9jX\")))\n",
    "                time.sleep(5)\n",
    "                self.data['category'].append(category)\n",
    "                self.data['id'].append(link.split(\"/\")[-1])\n",
    "                self.data['title'].append((content.find_element_by_class_name(\"VideoInfoTitle__videoTitle___11AcS\")).text)\n",
    "                self.data['description'].append((content.find_element_by_class_name(\"VideoInfoDescription__descriptionText___RB9jX\")).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_scrapper = Scraper()\n",
    "dailymotion_scrapper = Scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithreading to collect video link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['travel blog','science and technology', 'food', 'manufacturing', 'history','art and music']\n",
    "for category in categories:\n",
    "    t1 = threading.Thread(target=youtube_scrapper.get_links_youtube,args=(category,))\n",
    "    t2 = threading.Thread(target=dailymotion_scrapper.get_links_dailymotion,args=(category,))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t1.join()\n",
    "    t2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('list_youtube.pkl','rb') as f:\n",
    "    youtube_scrapper.links = pickle.load(f)\n",
    "with open('list_daily.pkl','rb') as f:\n",
    "    dailymotion_scrapper.links = pickle.load(f)\n",
    "\n",
    "youtube_scrapper.links[0:5]\n",
    "\n",
    "dailymotion_scrapper.links[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('list_youtube.pkl','wb') as f:\n",
    "    pickle.dump(youtube_scrapper.links,f)\n",
    "with open('list_daily.pkl','wb') as f:\n",
    "    pickle.dump(dailymotion_scrapper.links,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithreading to collect data related to videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-04da49534b0d>\", line 109, in get_data\n",
      "    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.Root__page___1mPV6\")))\n",
      "  File \"c:\\users\\udit\\assesmentprecily\\assesment\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py\", line 80, in until\n",
      "    raise TimeoutException(message, screen, stacktrace)\n",
      "selenium.common.exceptions.TimeoutException: Message: \n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\udit\\appdata\\local\\programs\\python\\python37-32\\Lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\udit\\appdata\\local\\programs\\python\\python37-32\\Lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-2-04da49534b0d>\", line 111, in get_data\n",
      "    self.driver.get(self.links[self.links.index(link) + 1])\n",
      "ValueError: 'https://www.dailymotion.com/video/x63evo2' is not in list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t3 = threading.Thread(target=youtube_scrapper.get_data,args=())\n",
    "t4 = threading.Thread(target=dailymotion_scrapper.get_data,args=())\n",
    "t3.start()\n",
    "t4.start()\n",
    "t3.join()\n",
    "t4.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = dailymotion_scrapper.data\n",
    "data_2 = youtube_scrapper.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pprint(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame.from_dict(data_1)\n",
    "df_2 = pd.DataFrame.from_dict(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.append(df_2,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl','wb') as f:\n",
    "    pickle.dump(df_1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "youtube_scrapper.data['id'] = []\n",
    "youtube_scrapper.data['title'] = []\n",
    "youtube_scrapper.data['category'] = []\n",
    "youtube_scrapper.data['description'] = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
